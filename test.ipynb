{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b73e57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset:Loaded 5 samples from data/processed/train_split.json\n",
      "INFO:src.data.dataset:Attempting to open video: data\\train\\videos\\5e4cb287_238_clip_003_0017_0026_Y.mp4\n",
      "INFO:src.data.dataset:Video file size: 27751640 bytes\n",
      "INFO:src.data.dataset:Video info - Total frames: 270, FPS: 30.0, Duration: 9.00s, Resolution: 2560x1440\n",
      "INFO:src.data.dataset:Frame indices to extract: [0, 44, 89, 134, 149, 179, 224, 269]\n",
      "INFO:src.data.dataset:Extracted 8 frames\n",
      "INFO:src.data.dataset:Saved frame to frame\\5e4cb287_238_clip_003_0017_0026_Y_0.png with shape (1440, 2560, 3)\n",
      "INFO:src.data.dataset:Saved frame to frame\\5e4cb287_238_clip_003_0017_0026_Y_1.png with shape (1440, 2560, 3)\n",
      "INFO:src.data.dataset:Saved frame to frame\\5e4cb287_238_clip_003_0017_0026_Y_2.png with shape (1440, 2560, 3)\n",
      "INFO:src.data.dataset:Saved frame to frame\\5e4cb287_238_clip_003_0017_0026_Y_3.png with shape (1440, 2560, 3)\n",
      "INFO:src.data.dataset:Saved frame to frame\\5e4cb287_238_clip_003_0017_0026_Y_4.png with shape (1440, 2560, 3)\n",
      "INFO:src.data.dataset:Saved frame to frame\\5e4cb287_238_clip_003_0017_0026_Y_5.png with shape (1440, 2560, 3)\n",
      "INFO:src.data.dataset:Saved frame to frame\\5e4cb287_238_clip_003_0017_0026_Y_6.png with shape (1440, 2560, 3)\n",
      "INFO:src.data.dataset:Saved frame to frame\\5e4cb287_238_clip_003_0017_0026_Y_7.png with shape (1440, 2560, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1440, 2560, 3)\n"
     ]
    }
   ],
   "source": [
    "from src.data.dataset import VietnameseTrafficDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def test_dataloader(\n",
    "    split_json: str,\n",
    "    video_root: str,\n",
    "    num_samples: int = 5,\n",
    "):\n",
    "    dataset = VietnameseTrafficDataset(\n",
    "        json_path=split_json,\n",
    "        video_root=video_root,\n",
    "        num_frames=8,\n",
    "        use_support_frames=True,\n",
    "        max_samples=num_samples\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return dataset, dataloader\n",
    "\n",
    "dataset, dataloader = test_dataloader(\n",
    "    split_json='data/processed/train_split.json',\n",
    "    video_root='data/',\n",
    "    num_samples=5\n",
    ")\n",
    "result = dataset.__getitem__(3)\n",
    "print(result['frames'].shape)  # Should be (8, H, W, 3) where H, W are video's original size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ccb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset:Loaded 2 samples from data/processed/train_split.json\n",
      "INFO:src.models.video_llava:LITE MODE: Using Qwen2-VL-2B for weaker machines\n",
      "INFO:src.models.video_llava:Loading model: Qwen/Qwen2-VL-2B-Instruct\n",
      "INFO:src.models.video_llava:Using 4-bit quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6086eadaf0264145b6c5e0d3ad63e17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.video_llava:Applying LoRA...\n",
      "INFO:src.models.video_llava:Model loaded successfully!\n",
      "INFO:src.data.dataset:Attempting to open video: data\\train\\videos\\train\\videos\\a7cad415_033_clip_003_0017_0024_N.mp4\n",
      "ERROR:src.data.dataset:Video file does not exist: data\\train\\videos\\train\\videos\\a7cad415_033_clip_003_0017_0024_N.mp4\n",
      "INFO:src.models.video_llava:Processor output keys: KeysView({'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 151652, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151653,     34, 124692, 129087,     25,   1163,    644,   2766,\n",
      "             11,  28776, 128415, 128283,  78228, 130027, 128603,    272, 126424,\n",
      "         128352,  32154, 128358,   5267,     34,  29341, 129506, 128509,    510,\n",
      "             32,     13,  12100,  86882,    272, 126424, 138239, 131149,    198,\n",
      "             33,     13,  12100,  86882,    272, 126424,  14854,  71431,     11,\n",
      "          12100,  86882,    272, 126424, 138239, 131149,    198,     34,     13,\n",
      "          12100,  86882,    272, 126424,  14854,  71431,     11,  12100,  86882,\n",
      "            272, 126424,    435, 124760, 130641,    198,     35,     13,  12100,\n",
      "          86882,    272, 126424,  14854,  71431,     11,  12100,  86882,    272,\n",
      "         126424, 138251,  47742,  14854,  71431,     11,  12100,  70248,    272,\n",
      "         126424,    435, 124760, 128282,    198,     39,   3202,     88, 128509,\n",
      "         130925, 128595, 129315,    320,     32,     11,    425,     11,    356,\n",
      "             11, 128475,    422,   1648, 151645,    198, 151644,  77091,    198,\n",
      "             35,     13,  12100,  86882,    272, 126424,  14854,  71431,     11,\n",
      "          12100,  86882,    272, 126424, 138251,  47742,  14854,  71431,     11,\n",
      "          12100,  70248,    272, 126424,    435, 124760, 128282, 151645,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]]), 'pixel_values_videos': tensor([[-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        ...,\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802]]), 'video_grid_thw': tensor([[ 4, 16, 16]])})\n",
      "INFO:src.models.video_llava:Input_ids shape: torch.Size([1, 414])\n",
      "INFO:src.models.video_llava:Pixel values videos shape: torch.Size([1024, 1176])\n",
      "INFO:src.models.video_llava:Video grid thw shape: torch.Size([1, 3])\n",
      "INFO:src.models.video_llava:Labels shape: torch.Size([1, 414])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,232,384 || all params: 2,218,217,984 || trainable%: 0.4162\n",
      "Frames shape: torch.Size([1, 8, 3, 224, 224])\n",
      "Prompts: ['Câu hỏi: Trong video, có xuất hiện các biển báo cấm nào sau đây?\\nCác lựa chọn:\\nA. Biển cấm ngược chiều\\nB. Biển cấm đỗ, Biển cấm ngược chiều\\nC. Biển cấm đỗ, Biển cấm rẽ trái\\nD. Biển cấm đỗ, Biển cấm dừng và đỗ, Biến cấm rẽ phải\\nHãy chọn đáp án đúng (A, B, C, hoặc D):']\n",
      "Answers: ['D. Biển cấm đỗ, Biển cấm dừng và đỗ, Biến cấm rẽ phải']\n",
      "Input keys: KeysView({'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 151652, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
      "         151656, 151653,     34, 124692, 129087,     25,   1163,    644,   2766,\n",
      "             11,  28776, 128415, 128283,  78228, 130027, 128603,    272, 126424,\n",
      "         128352,  32154, 128358,   5267,     34,  29341, 129506, 128509,    510,\n",
      "             32,     13,  12100,  86882,    272, 126424, 138239, 131149,    198,\n",
      "             33,     13,  12100,  86882,    272, 126424,  14854,  71431,     11,\n",
      "          12100,  86882,    272, 126424, 138239, 131149,    198,     34,     13,\n",
      "          12100,  86882,    272, 126424,  14854,  71431,     11,  12100,  86882,\n",
      "            272, 126424,    435, 124760, 130641,    198,     35,     13,  12100,\n",
      "          86882,    272, 126424,  14854,  71431,     11,  12100,  86882,    272,\n",
      "         126424, 138251,  47742,  14854,  71431,     11,  12100,  70248,    272,\n",
      "         126424,    435, 124760, 128282,    198,     39,   3202,     88, 128509,\n",
      "         130925, 128595, 129315,    320,     32,     11,    425,     11,    356,\n",
      "             11, 128475,    422,   1648, 151645,    198, 151644,  77091,    198,\n",
      "             35,     13,  12100,  86882,    272, 126424,  14854,  71431,     11,\n",
      "          12100,  86882,    272, 126424, 138251,  47742,  14854,  71431,     11,\n",
      "          12100,  70248,    272, 126424,    435, 124760, 128282, 151645,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]]), 'pixel_values_videos': tensor([[-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        ...,\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802]]), 'video_grid_thw': tensor([[ 4, 16, 16]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "             35,     13,  12100,  86882,    272, 126424,  14854,  71431,     11,\n",
      "          12100,  86882,    272, 126424, 138251,  47742,  14854,  71431,     11,\n",
      "          12100,  70248,    272, 126424,    435, 124760, 128282, 151645,    198]])})\n",
      "Input_ids shape: torch.Size([1, 414])\n",
      "Loss: 0.06418251246213913\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from pathlib import Path\n",
    "# from src.data.dataset import VietnameseTrafficDataset, collate_fn\n",
    "# from src.models.video_llava import VietnameseTrafficVQAModel\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Load a small subset\n",
    "# dataset = VietnameseTrafficDataset(\n",
    "#     json_path=\"data/processed/train_split.json\",\n",
    "#     video_root=\"data/train/videos\",\n",
    "#     num_frames=8,\n",
    "#     max_samples=2  # Just 2 samples for testing\n",
    "# )\n",
    "\n",
    "# # Create dataloader\n",
    "# loader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)\n",
    "\n",
    "# # Initialize model\n",
    "# model = VietnameseTrafficVQAModel(\n",
    "#     mode=\"lite\",  # Use lite for testing\n",
    "#     use_lora=True,\n",
    "#     load_in_4bit=True\n",
    "# )\n",
    "\n",
    "# # Test forward pass\n",
    "# for batch in loader:\n",
    "#     frames = batch['frames']\n",
    "#     prompts = batch['prompts']\n",
    "#     answers = batch['answers']\n",
    "    \n",
    "#     print(f\"Frames shape: {frames.shape}\")\n",
    "#     print(f\"Prompts: {prompts}\")\n",
    "#     print(f\"Answers: {answers}\")\n",
    "    \n",
    "#     # Test prepare_inputs\n",
    "#     inputs = model.prepare_inputs(frames, prompts, answers)\n",
    "#     print(f\"Input keys: {inputs.keys()}\")\n",
    "#     print(f\"Input_ids shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "#     # Test forward pass\n",
    "#     inputs = {k: v.to('cpu') if isinstance(v, torch.Tensor) else v \n",
    "#               for k, v in inputs.items()}\n",
    "#     outputs = model(**inputs)\n",
    "#     print(f\"Loss: {outputs.loss.item()}\")\n",
    "    \n",
    "#     break  # Just test one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a38c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roadbuddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
